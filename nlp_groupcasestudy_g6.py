# -*- coding: utf-8 -*-
"""NLP_GroupCaseStudy_G6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A9KjptNZsYpdgl-3oWGsqa31cdgf1v8e
"""

# Import Libraries
#nltk.download('all')
import numpy as np
from sklearn.model_selection import learning_curve

import pandas as pd
import nltk

from nltk.stem import PorterStemmer
from nltk import word_tokenize, pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import  CountVectorizer, TfidfVectorizer
from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt





# Load the dataset
df = pd.read_csv("/content/mental_health.csv")

# Print the datatypes of columns
print(df.dtypes)

# Data Preprocessing
# Removing special characters for Problem_description
msg = df.text.str.replace('[^a-zA-Z0-9]+'," ")
print(msg)


# Converting text to lower case for Problem_description
df['text'] = df['text'].str.lower()

print(df.head())


# Tokenization for Problem_description
word = nltk.word_tokenize(df["text"][0])
print(word)


# Stemming for Problem_description
stemer = PorterStemmer()
print(stopwords.words('english'))


# Lemmatization for Problem_description
lemmatizer = WordNetLemmatizer()
print(stopwords.words('english'))



# Removing special characters for problem_summary
msg1 = df.text.str.replace('[^a-zA-Z0-9]+'," ")
print(msg1)


# Converting text to lower case for problem_summary
df['text'] = df['text'].str.lower()

print(df.head())


# Tokenization for problem_summary
words = nltk.word_tokenize(df["text"][0])
print(words)


# Stemming for problem_summary
stemer = PorterStemmer()
print(stopwords.words('english'))


# Lemmatization for problem_summary
lemmatizer = WordNetLemmatizer()
print(stopwords.words('english'))


# Split data into features (X) and target variable (y)
X = df['text']
y = df['label']


print(X.head())  # Print the first few rows of X
print(y.head()) # Print the first few rows of y



# TF - IDF model
cv = TfidfVectorizer()
corpus = df['text']
x = cv.fit_transform(corpus)
print(x.shape)


# Split data into training and testing set
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)



imputer = SimpleImputer(strategy='most_frequent')  # You can choose a suitable imputation strategy
y_train_imputed = imputer.fit_transform(y_train.values.reshape(-1, 1))
y_train = pd.Series(y_train_imputed.flatten(), index=y_train.index)


# Define classifiers
classifiers = [MultinomialNB(),
               RandomForestClassifier(),
               KNeighborsClassifier(),
               SVC()]

# Model Evaluation
precision = []
recall = []
f1_score_list = []
trainset_accuracy = []
testset_accuracy = []

for cls in classifiers:
    cls.fit(X_train, y_train)
    pred_train = cls.predict(X_train)
    pred_test = cls.predict(X_test)
    prec = precision_score(y_test, pred_test, average='weighted')
    recal = recall_score(y_test, pred_test, average='weighted')
    f1_s = f1_score(y_test, pred_test, average='weighted')
    train_accuracy = accuracy_score(y_train, pred_train)
    test_accuracy = accuracy_score(y_test, pred_test)

    # Appending scores
    precision.append(prec)
    recall.append(recal)
    f1_score_list.append(f1_s)
    trainset_accuracy.append(train_accuracy)
    testset_accuracy.append(test_accuracy)

# Create DataFrame for results
data = {
    'Precision': precision,
    'Recall': recall,
    'F1score': f1_score_list,
    'Accuracy on Testset': testset_accuracy,
    'Accuracy on Trainset': trainset_accuracy
}
Results = pd.DataFrame(data, index=["NaiveBayes", "RandomForest", "KNeighbours", "SVC"])
print(Results)

# Visualize

plt.figure(figsize=(10, 6))
sns.barplot(x=Results.index, y='Accuracy on Testset', data=Results)
plt.title('Accuracy on Testset for Different Models')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Line plot for Model Evaluation Metrics
plt.figure(figsize=(10, 6))
plt.plot(Results.index, Results['Precision'], marker='o', label='Precision')
plt.plot(Results.index, Results['Recall'], marker='o', label='Recall')
plt.plot(Results.index, Results['F1score'], marker='o', label='F1 Score')
plt.title('Model Evaluation Metrics Comparison')
plt.xlabel('Model')
plt.ylabel('Score')
plt.legend()
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()


#learning curve
plt.figure(figsize=(10, 6))
for cls in classifiers:
    train_sizes, train_scores, test_scores = learning_curve(cls, X_train, y_train, cv=5)
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.plot(train_sizes, train_mean, marker='o', label='Training Score')
    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15)
    plt.plot(train_sizes, test_mean, marker='o', label='Cross-Validation Score')
    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.15)

plt.title('Learning Curves for Different Models')
plt.xlabel('Training Examples')
plt.ylabel('Score')
plt.legend(Results.index)
plt.grid(True)
plt.show()


# ROC curve
plt.figure(figsize=(10, 6))
for cls in classifiers:
    cls.fit(X_train, y_train)
    if hasattr(cls, "predict_proba"):  # Check if classifier supports predict_proba
        y_scores = cls.predict_proba(X_test)[:, 1]  # Get predicted probabilities for positive class
    elif hasattr(cls, "decision_function"):  # Check if classifier supports decision_function
        y_scores = cls.decision_function(X_test)
    else:
        raise AttributeError("Classifier does not support predict_proba or decision_function.")
    fpr, tpr, _ = metrics.roc_curve(y_test, y_scores)  # Compute ROC curve
    plt.plot(fpr, tpr, marker='.', label=str(cls))  # Plot ROC curve
plt.title('ROC Curve for Different Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.plot([0, 1], [0, 1], linestyle='--', color='black')
plt.grid(True)
plt.show()

# Precision-recall curve
plt.figure(figsize=(10, 6))
for cls in classifiers:
    cls.fit(X_train, y_train)
    if hasattr(cls, "predict_proba"):  # Check if classifier supports predict_proba
        y_scores = cls.predict_proba(X_test)[:, 1]  # Get predicted probabilities for positive class
    elif hasattr(cls, "decision_function"):  # Check if classifier supports decision_function
        y_scores = cls.decision_function(X_test)
    else:
        raise AttributeError("Classifier does not support predict_proba or decision_function.")
    precision, recall, _ = metrics.precision_recall_curve(y_test, y_scores)  # Compute precision-recall pairs
    plt.plot(recall, precision, marker='.', label=str(cls))  # Plot precision-recall curve
plt.title('Precision-Recall Curve for Different Models')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)
plt.show()


# Define colors for each bar
colors = sns.color_palette('pastel')

# Visualize Precision, Recall, F1 Score, Accuracy on Test Set, and Accuracy on Train Set for Different Models
# Melt the Results DataFrame
melted_results = Results.reset_index().melt(id_vars='index', var_name='Metric', value_name='Value')

# Define colors for each metric
colors = sns.color_palette('husl', len(melted_results['Metric'].unique()))

# Plotting
plt.figure(figsize=(14, 10))
sns.barplot(x='index', y='Value', hue='Metric', data=melted_results, palette=colors)
plt.title('Model Performance Comparison')
plt.xlabel('Model')
plt.ylabel('Metric Value')
plt.legend(title='Metric', loc='upper right')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

